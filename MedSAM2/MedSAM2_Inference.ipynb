{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ede253-7a9c-4a66-8cba-8d88ed48f7b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create the environment\n",
    "!conda env create -f environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0689c389-b157-4ac6-b5aa-be7a89fa77c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# conda activate medSAM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a5722-822c-4df7-8029-d4754d581086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21ea8fd-40a4-437f-9871-9475144bb9ec",
   "metadata": {},
   "source": [
    "### Download MedSAM2 Weights and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34fbc55-c385-4c68-a959-d7a051659f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://huggingface.co/jiayuanz3/MedSAM2_pretrain/resolve/main/MedSAM2_pretrain.pth?download=true\"  # Replace with the actual URL\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(\"MedSAM2_pretrain.pth\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(\"File downloaded successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download file. HTTP Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44ed744-e394-4977-868b-95af26cae25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0c8fdd-684f-4320-b82f-b250ea8c2777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "\n",
    "# Function to generate random points inside a bounding box and round them\n",
    "def generate_random_points(bbox, num_points=10, decimal_places=0):\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    points = []\n",
    "    labels = []  # Corresponding labels for the points\n",
    "    for _ in range(num_points):\n",
    "        # Generate random x and y coordinates inside the bounding box\n",
    "        x = random.uniform(x_min, x_max)\n",
    "        y = random.uniform(y_min, y_max)\n",
    "        \n",
    "        # Round the coordinates to the specified number of decimal places\n",
    "        points.append([round(x, decimal_places), round(y, decimal_places)])\n",
    "        \n",
    "        # Assign label '1' for all points (you can modify this if needed)\n",
    "        labels.append(1)\n",
    "    \n",
    "    return points, labels\n",
    "\n",
    "# Function to get bounding box\n",
    "def get_bounding_box(ground_truth_map):\n",
    "    ground_truth_map = np.array(ground_truth_map)\n",
    "    y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "    # print(y_indices, x_indices)\n",
    "    if len(y_indices) == 0 or len(x_indices) == 0:  # If there are no non-zero elements\n",
    "        return None\n",
    "    x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "    y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "    # Add perturbation to bounding box coordinates\n",
    "    H, W = ground_truth_map.shape\n",
    "    x_min = max(0, x_min - np.random.randint(0, 5))\n",
    "    x_max = min(W, x_max + np.random.randint(0, 5))\n",
    "    y_min = max(0, y_min - np.random.randint(0, 5))\n",
    "    y_max = min(H, y_max + np.random.randint(0, 5))\n",
    "    bbox = [x_min, y_min, x_max, y_max]\n",
    "    \n",
    "    return bbox\n",
    "\n",
    "# Function to get bounding boxes for a batch\n",
    "def get_bounding_boxes_for_batch(batch_masks):\n",
    "    bounding_boxes = []\n",
    "    for i in range(batch_masks.shape[0]):\n",
    "        bbox = get_bounding_box(batch_masks[i])\n",
    "        if bbox:\n",
    "            bounding_boxes.append(bbox)\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22bb859-d896-4825-be21-6ac56f6179d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# If running in a Jupyter notebook, strip out the '-f' argument\n",
    "if 'ipykernel_launcher' in sys.argv[0]:\n",
    "    sys.argv = sys.argv[:1]\n",
    "\n",
    "# Continue with your normal imports\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import cfg\n",
    "import func_2d.function as function\n",
    "from conf import settings\n",
    "from func_2d.dataset import *\n",
    "from func_2d.utils import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc97ab-ad96-4f9c-8356-460f74e12a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use bfloat16 for the entire work\n",
    "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "if torch.cuda.get_device_properties(0).major >= 8:\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041a92d9-2135-47cc-81d3-e4ba1b765374",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = cfg.parse_args()\n",
    "GPUdevice = torch.device('cuda', args.gpu_device)\n",
    "\n",
    "\n",
    "args.pretrain = './MedSAM2_pretrain.pth'\n",
    "args.net = 'sam2'\n",
    "args.exp_name = 'REFUGE_MedSAM2'\n",
    "args.vis = 1\n",
    "args.sam_ckpt = './checkpoints/sam2_hiera_tiny.pt'\n",
    "args.sam_config = 'sam2_hiera_t'\n",
    "args.image_size = 1024\n",
    "args.out_size = 1024\n",
    "# args.batch_size = 16\n",
    "args.val_freq = 1\n",
    "# args.dataset = 'REFUGE'\n",
    "# args.data_path = './data/REFUGE'\n",
    "print(args)  # Check if 'model.to' exists in the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb99ef5-c804-4899-9d51-6bed47808672",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = get_network(args, args.net, use_gpu=args.gpu, gpu_device=GPUdevice, distribution = \"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad8b8d-8689-439b-9a05-f49fddcca6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, checkpoint_path, device='cuda:0'):\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    print(\"Checkpoint Keys:\", checkpoint.keys())\n",
    "    \n",
    "    # Load the state dict from the checkpoint\n",
    "    checkpoint_dict = checkpoint['model']\n",
    "    \n",
    "    # Get the model's state dict\n",
    "    model_dict = model.state_dict()\n",
    "    \n",
    "    # Count the total number of layers in the checkpoint and the model\n",
    "    total_checkpoint_layers = len(checkpoint_dict)\n",
    "    total_model_layers = len(model_dict)\n",
    "    \n",
    "    # Filter out incompatible layers from the checkpoint and only keep the matching ones\n",
    "    compatible_checkpoint_dict = {k: v for k, v in checkpoint_dict.items() if k in model_dict and v.shape == model_dict[k].shape}\n",
    "    \n",
    "    # Count the number of matching layers\n",
    "    matching_layers = len(compatible_checkpoint_dict)\n",
    "    \n",
    "    print(f\"Total layers in checkpoint: {total_checkpoint_layers}\")\n",
    "    print(f\"Total layers in model: {total_model_layers}\")\n",
    "    print(f\"Found {matching_layers} matching layers.\")\n",
    "    \n",
    "    # Update model's state dict with the compatible layers from the checkpoint\n",
    "    model_dict.update(compatible_checkpoint_dict)\n",
    "    \n",
    "    # Load the updated state dict into the model\n",
    "    model.load_state_dict(model_dict, strict=False)\n",
    "    \n",
    "    print(\"Checkpoint loaded successfully!\")\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "net = load_checkpoint(net, './MedSAM2_pretrain.pth', device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092df0dc-0074-483a-841b-194c7cbd3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "import os\n",
    "import random\n",
    "import cv2  # For elastic deformations\n",
    "\n",
    "def preprocess_sample(image_path, mask_path, image_mode='L', mask_mode='L', target_size=(1024, 1024)):\n",
    "    \n",
    "    # Crop and resize\n",
    "    def crop_and_resize(image, mask, target_size):\n",
    "        image_array = np.array(image)\n",
    "        mask_array = np.array(mask)\n",
    "        \n",
    "        non_zero_rows = np.any(image_array, axis=1)\n",
    "        non_zero_cols = np.any(image_array, axis=0)\n",
    "        \n",
    "        row_indices = np.where(non_zero_rows)[0]\n",
    "        col_indices = np.where(non_zero_cols)[0]\n",
    "        \n",
    "        # print(mask_array.shape)\n",
    "        if len(row_indices) > 0 and len(col_indices) > 0:\n",
    "            cropped_image = image_array[min(row_indices):max(row_indices), min(col_indices):max(col_indices)]\n",
    "            cropped_mask = mask_array[min(row_indices):max(row_indices), min(col_indices):max(col_indices)]\n",
    "        else:\n",
    "            cropped_image = image_array\n",
    "            cropped_mask = mask_array\n",
    "            \n",
    "        # print(cropped_image)\n",
    "        cropped_image = Image.fromarray(cropped_image).resize(target_size, Image.BILINEAR)\n",
    "        cropped_mask = Image.fromarray(cropped_mask).resize(target_size, Image.BILINEAR)\n",
    "        return cropped_image, cropped_mask\n",
    "\n",
    "    image = Image.open(image_path).convert(image_mode)\n",
    "    mask = Image.open(mask_path).convert(mask_mode).resize(image.size, Image.BILINEAR)\n",
    "\n",
    "    cropped_image, cropped_mask = crop_and_resize(image, mask, target_size)\n",
    "    \n",
    "    image_tensor = torch.tensor(np.array(cropped_image), dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Add batch and channel dim\n",
    "    mask_tensor = torch.tensor(np.array(cropped_mask), dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    return image_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb349e-fdac-45ea-869d-8fa686ebfb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, file_list, image_dir, mask_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_list (list of str): List of filenames.\n",
    "            image_dir (str): Path to the directory containing the images.\n",
    "            mask_dir (str): Path to the directory containing the depth masks.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.file_list = file_list\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get filename from the list\n",
    "        filename = self.file_list[idx]\n",
    "        \n",
    "        # Construct full paths for the image and depth mask\n",
    "        image_path = os.path.join(self.image_dir, filename)\n",
    "        mask_path = os.path.join(self.mask_dir, filename)\n",
    "        \n",
    "        image_tensor, mask_tensor = preprocess_sample(image_path, mask_path)\n",
    "\n",
    "        mask_tensor = torch.where(mask_tensor > 0.0, 1.0, 0.0)  # Threshold at 0.5\n",
    "\n",
    "        return filename, image_tensor, mask_tensor\n",
    "\n",
    "# Function to read file and store lines in a list\n",
    "def read_file_to_list(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bccdaf7-e455-467f-9b97-c0e6917962fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "Datasets = [\"BraTS20\", \"BraTS21\", \"BraTS23\", \"MSD\"]\n",
    "\n",
    "data_names = []\n",
    "\n",
    "for dataset_name in Datasets:\n",
    "    file_list = os.listdir('./data/' + dataset_name + '/Image/')\n",
    "    image_dir = './data/' + dataset_name + '/Image/'\n",
    "    mask_dir = './data/' + dataset_name + '/GT/'\n",
    "    \n",
    "    # Create the dataset instance\n",
    "    dataset = MedicalDataset(file_list=file_list, image_dir=image_dir, mask_dir=mask_dir)\n",
    "    # Create the DataLoader\n",
    "    train_loader = DataLoader(dataset, batch_size = 1, shuffle=True)\n",
    "    filenames_list = []\n",
    "    # Example: Iterate over the DataLoader and print the batch shape\n",
    "    for batch_idx, (filename, images, masks) in enumerate(train_loader):\n",
    "        filename = filename\n",
    "        print(filename[0])\n",
    "        filenames_list.append(filename)\n",
    "        image_tensor = images\n",
    "        mask_tensor = masks\n",
    "\n",
    "        image_tensor = image_tensor.to(torch.bfloat16)  # Convert input to BFloat16\n",
    "        image_tensor = image_tensor.view(image_tensor.shape[0],1,1024,1024).cuda()  # Convert input to BFloat16\n",
    "        image_tensor = image_tensor.repeat(1, 3, 1, 1)\n",
    "        mask_tensor = mask_tensor.view(image_tensor.shape[0],1024,1024)#cuda()  # Convert input to BFloat16\n",
    "\n",
    "        input_boxes = get_bounding_boxes_for_batch(mask_tensor)\n",
    "        # Generate random points for each bounding box and store them in the desired format\n",
    "        batch_random_points = []\n",
    "        batch_random_points_labels = []\n",
    "        \n",
    "        # For each bounding box, generate points and append to the batch\n",
    "        for bbox in input_boxes:\n",
    "            points, labels = generate_random_points(bbox)  # Generate 10 points and labels for each bounding box\n",
    "            batch_random_points.append([points])  # [batchsize, num_points, point_location, labels]\n",
    "            batch_random_points_labels.append([labels])  # [batchsize, num_points, point_location, labels]\n",
    "        \n",
    "        coords_torch = torch.as_tensor(batch_random_points, dtype=torch.float, device=GPUdevice).squeeze(1)\n",
    "        labels_torch = torch.as_tensor(batch_random_points_labels, dtype=torch.int, device=GPUdevice).squeeze(1)\n",
    "\n",
    "        memory_bank_list = []\n",
    "        feat_sizes = [(256, 256), (128,128), (64,64)]\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            \"\"\" image encoder \"\"\"\n",
    "            backbone_out = net.forward_image(image_tensor)\n",
    "            _, vision_feats, vision_pos_embeds, _ = net._prepare_backbone_features(backbone_out)\n",
    "            B = vision_feats[-1].size(1) \n",
    "        \n",
    "            \"\"\" memory condition \"\"\"\n",
    "            if len(memory_bank_list) == 0:\n",
    "                vision_feats[-1] = vision_feats[-1] + torch.nn.Parameter(torch.zeros(1, B, net.hidden_dim)).to(device=\"cuda\")\n",
    "                vision_pos_embeds[-1] = vision_pos_embeds[-1] + torch.nn.Parameter(torch.zeros(1, B, net.hidden_dim)).to(device=\"cuda\")\n",
    "        \n",
    "            else:\n",
    "                for element in memory_bank_list:\n",
    "                    maskmem_features = element[0]\n",
    "                    maskmem_pos_enc = element[1]\n",
    "                    to_cat_memory.append(maskmem_features.cuda(non_blocking=True).flatten(2).permute(2, 0, 1))\n",
    "                    to_cat_memory_pos.append(maskmem_pos_enc.cuda(non_blocking=True).flatten(2).permute(2, 0, 1))\n",
    "                    to_cat_image_embed.append((element[3]).cuda(non_blocking=True)) # image_embed\n",
    "                    \n",
    "                memory_stack_ori = torch.stack(to_cat_memory, dim=0)\n",
    "                memory_pos_stack_ori = torch.stack(to_cat_memory_pos, dim=0)\n",
    "                image_embed_stack_ori = torch.stack(to_cat_image_embed, dim=0)\n",
    "        \n",
    "                vision_feats_temp = vision_feats[-1].permute(1, 0, 2).view(B, -1, 64, 64) \n",
    "                vision_feats_temp = vision_feats_temp.reshape(B, -1)\n",
    "        \n",
    "                image_embed_stack_ori = F.normalize(image_embed_stack_ori, p=2, dim=1)\n",
    "                vision_feats_temp = F.normalize(vision_feats_temp, p=2, dim=1)\n",
    "                similarity_scores = torch.mm(image_embed_stack_ori, vision_feats_temp.t()).t()\n",
    "        \n",
    "                similarity_scores = F.softmax(similarity_scores, dim=1) \n",
    "                sampled_indices = torch.multinomial(similarity_scores, num_samples=B, replacement=True).squeeze(1)  # Shape [batch_size, 16]\n",
    "        \n",
    "                memory_stack_ori_new = (memory_stack_ori[sampled_indices].squeeze(3).permute(1, 2, 0, 3))\n",
    "                memory = memory_stack_ori_new.reshape(-1, memory_stack_ori_new.size(2), memory_stack_ori_new.size(3))\n",
    "        \n",
    "                memory_pos_stack_new = (memory_pos_stack_ori[sampled_indices].squeeze(3).permute(1, 2, 0, 3))\n",
    "                memory_pos = memory_pos_stack_new.reshape(-1, memory_stack_ori_new.size(2), memory_stack_ori_new.size(3))\n",
    "        \n",
    "        \n",
    "        \n",
    "                vision_feats[-1] = net.memory_attention(\n",
    "                    curr=[vision_feats[-1]],\n",
    "                    curr_pos=[vision_pos_embeds[-1]],\n",
    "                    memory=memory,\n",
    "                    memory_pos=memory_pos,\n",
    "                    num_obj_ptr_tokens=0\n",
    "                    )\n",
    "            \n",
    "            # print(\"feat:\", feat)\n",
    "            # print(\"vision_feats[::-1][0] shape:\", vision_feats[::-1][0].shape)\n",
    "            # print(\"vision_feats[::-1][1] shape:\", vision_feats[::-1][1].shape)\n",
    "            # print(\"vision_feats[::-1][2] shape:\", vision_feats[::-1][2].shape)\n",
    "        \n",
    "            # feats = [feat.permute(1, 2, 0).view(B, -1, *feat_size) \n",
    "            #         for feat, feat_size in zip(vision_feats[::-1], feat_sizes[::-1])][::-1]\n",
    "            \n",
    "            feats = [feat.permute(1, 2, 0).reshape(B, -1, *feat_size) \n",
    "                     for feat, feat_size in zip(vision_feats[::-1], feat_sizes[::-1])][::-1]\n",
    "            \n",
    "            image_embed = feats[-1]\n",
    "            high_res_feats = feats[:-1]\n",
    "        \n",
    "            \"\"\" prompt encoder \"\"\"\n",
    "            points = (coords_torch, labels_torch)\n",
    "        \n",
    "            \n",
    "            se, de = net.sam_prompt_encoder(\n",
    "                points=points, \n",
    "                boxes=None,\n",
    "                masks=None,\n",
    "                batch_size=B,\n",
    "            )\n",
    "        \n",
    "            # print(\"image embedding:\", image_embed.shape)\n",
    "            # print(\"sparse embedding:\", se.shape)\n",
    "            # print(\"dense embedding:\", de.shape)\n",
    "        \n",
    "                  \n",
    "            low_res_multimasks, iou_predictions, sam_output_tokens, object_score_logits = net.sam_mask_decoder(\n",
    "                image_embeddings=image_embed,\n",
    "                image_pe=net.sam_prompt_encoder.get_dense_pe(), \n",
    "                sparse_prompt_embeddings=se,\n",
    "                dense_prompt_embeddings=de, \n",
    "                multimask_output=False, \n",
    "                repeat_image=False,  \n",
    "                high_res_features = high_res_feats\n",
    "            )\n",
    "        \n",
    "            # prediction\n",
    "            pred = F.interpolate(low_res_multimasks,size=(args.out_size,args.out_size))\n",
    "            high_res_multimasks = F.interpolate(low_res_multimasks, size=(args.image_size, args.image_size),\n",
    "                                            mode=\"bilinear\", align_corners=False)\n",
    "            \n",
    "        print(f\":::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\")\n",
    "        print(f\":::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\")\n",
    "        # Assuming you have tensors: image, mask, pred with shape [16, 1, 1024, 1024]\n",
    "        # Select the first sample (index 0) for visualization\n",
    "        for j in range(image_tensor.shape[0]):\n",
    "            image_sample = image_tensor[j, 0].to(torch.float32).cpu().numpy()  # (1024, 1024)\n",
    "            mask_sample = mask_tensor[j].to(torch.float32).cpu().numpy()    # (1024, 1024)\n",
    "        \n",
    "            pred_sample_sig = torch.sigmoid(high_res_multimasks[j, 0])\n",
    "            pred_sample_norm = pred_sample_sig / pred_sample_sig.max()\n",
    "            # pred_sample_norm = 1 - pred_sample_norm\n",
    "            # print(pred_sample_norm.min(), pred_sample_norm.max())\n",
    "            pred_sample_binary = torch.where(pred_sample_norm > 0.0, 1.0, 0.0).squeeze(0)\n",
    "            pred_sample_binary = 1 - pred_sample_binary\n",
    "            # print(pred_sample_sig.min(), pred_sample_sig.max(), pred_sample_sig.unique())\n",
    "            pred_sample_np = pred_sample_binary.to(torch.float32).cpu().numpy()  # (1024, 1024)\n",
    "            \n",
    "            # Create a plot with 3 subplots (side by side)\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            \n",
    "            # Display the image\n",
    "            axes[0].imshow(image_sample, cmap='gray')\n",
    "            axes[0].set_title('Image')\n",
    "            axes[0].axis('off')  # Hide axis\n",
    "            \n",
    "            # Display the mask\n",
    "            axes[1].imshow(mask_sample, cmap='gray')\n",
    "            axes[1].set_title('Mask')\n",
    "            axes[1].axis('off')  # Hide axis\n",
    "            \n",
    "            # Display the prediction\n",
    "            axes[2].imshow(pred_sample_np, cmap='gray')\n",
    "            axes[2].set_title('Prediction')\n",
    "            axes[2].axis('off')  # Hide axis\n",
    "            \n",
    "            # Show the plot\n",
    "            plt.suptitle(\n",
    "                str(dataset_name),\n",
    "                fontsize=20,  # Font size\n",
    "                fontweight='bold',  # Font weight (e.g., 'normal', 'bold')\n",
    "                color=\"red\"  # Font color\n",
    "            )\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "        break\n",
    "    data_names.append(filenames_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2700606c-eab0-4644-ab2c-d19fbdd99be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_names\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "del net\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab10be8-08af-4a2d-8159-3fb1568cb96a",
   "metadata": {},
   "source": [
    "## MedSAM Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1f07bc-c29b-44af-a450-5a7aff90a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from transformers import SamModel, SamProcessor\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = SamModel.from_pretrained(\"flaviagiammarino/medsam-vit-base\").to(device)\n",
    "processor = SamProcessor.from_pretrained(\"flaviagiammarino/medsam-vit-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcd3924-d422-4212-a77c-4d5d5c8a54ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "Datasets = [\"BraTS20\", \"BraTS21\", \"BraTS23\", \"MSD\"]\n",
    "\n",
    "for i in range(len(Datasets)):\n",
    "    # print(list(data_names[i][0]))\n",
    "    # file_list = list(data_names[i][0])\n",
    "    file_list = os.listdir('./data/' + Datasets[i] + '/Image/')\n",
    "    image_dir = './data/' + Datasets[i] + '/Image/'\n",
    "    mask_dir = './data/' + Datasets[i] + '/GT/'\n",
    "    \n",
    "    # Create the dataset instance\n",
    "    dataset = MedicalDataset(file_list=file_list, image_dir=image_dir, mask_dir=mask_dir)\n",
    "    \n",
    "    # Create the DataLoader\n",
    "    train_loader = DataLoader(dataset, batch_size = 1, shuffle=True)\n",
    "    \n",
    "    # Example: Iterate over the DataLoader and print the batch shape\n",
    "    for batch_idx, (filenames, images, masks) in enumerate(train_loader):\n",
    "        image_tensor = images\n",
    "        mask_tensor = masks\n",
    "\n",
    "        image_tensor = image_tensor.to(torch.bfloat16)  # Convert input to BFloat16\n",
    "        image_tensor = image_tensor.view(image_tensor.shape[0],1,1024,1024).cuda()  # Convert input to BFloat16\n",
    "        image_tensor = image_tensor.repeat(1, 3, 1, 1)\n",
    "        mask_tensor = mask_tensor.view(image_tensor.shape[0],1024,1024)#cuda()  # Convert input to BFloat16\n",
    "\n",
    "        input_boxes = get_bounding_boxes_for_batch(mask_tensor)\n",
    "        # Generate random points for each bounding box and store them in the desired format\n",
    "        batch_random_points = []\n",
    "        batch_random_points_labels = []\n",
    "        \n",
    "        # For each bounding box, generate points and append to the batch\n",
    "        for bbox in input_boxes:\n",
    "            points, labels = generate_random_points(bbox)  # Generate 10 points and labels for each bounding box\n",
    "            batch_random_points.append([points])  # [batchsize, num_points, point_location, labels]\n",
    "            batch_random_points_labels.append([labels])  # [batchsize, num_points, point_location, labels]\n",
    "        \n",
    "        # prepare image + box prompt for the model\n",
    "        inputs = processor(image_tensor.to(torch.float32), input_points=batch_random_points, input_labels = batch_random_points_labels, return_tensors=\"pt\").to(device)\n",
    "        # for k,v in inputs.items():\n",
    "          # print(k,v.shape)\n",
    "\n",
    "        image_embeddings = model.vision_encoder(inputs['pixel_values'])\n",
    "        image_embeddings = image_embeddings['last_hidden_state']\n",
    "\n",
    "        input_points = inputs.get('input_points', None)\n",
    "        input_labels = inputs.get('input_labels', None)\n",
    "        input_boxes = inputs.get('input_boxes', None)\n",
    "        input_masks = inputs.get('input_masks', None)\n",
    "        \n",
    "        sparse_embeddings, dense_embeddings = model.prompt_encoder(\n",
    "            input_points = input_points,\n",
    "            input_boxes = input_boxes,\n",
    "            input_masks = input_masks,\n",
    "            input_labels = input_labels\n",
    "        )\n",
    "\n",
    "        image_positional_embeddings = model.get_image_wide_positional_embeddings()\n",
    "        # repeat with batch size\n",
    "        batch_size = inputs['pixel_values'].shape[0] if inputs['pixel_values'] is not None else image_embeddings.shape[0]\n",
    "        image_positional_embeddings = image_positional_embeddings.repeat(batch_size, 1, 1, 1)\n",
    "        \n",
    "        low_res_masks, iou_predictions, mask_decoder_attentions = model.mask_decoder(image_embeddings = image_embeddings, image_positional_embeddings = image_positional_embeddings, \n",
    "                                    sparse_prompt_embeddings = sparse_embeddings, dense_prompt_embeddings = dense_embeddings, multimask_output = False)\n",
    "\n",
    "        medsam_seg_prob = torch.sigmoid(low_res_masks[0].squeeze(1)).squeeze(1)\n",
    "        preds = torch.where(medsam_seg_prob > 0.5, 1.0, 0.0)\n",
    "\n",
    "        img = image_tensor.to(torch.float32)        \n",
    "        tensor_resized = F.interpolate(img, size=(256,256), mode='bilinear', align_corners=False)\n",
    "\n",
    "        print(f\":::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\")\n",
    "        print(f\":::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\")\n",
    "        # Select the first sample (index 0) for visualization\n",
    "        for j in range(image_tensor.shape[0]):\n",
    "            image_sample = image_tensor[j, 0].to(torch.float32).cpu().numpy()  # (1024, 1024)\n",
    "            mask_sample = mask_tensor[j].to(torch.float32).cpu().numpy()    # (1024, 1024)\n",
    "            pred_sample = preds[j].cpu().numpy()  # (1024, 1024)\n",
    "            \n",
    "            # Create a plot with 3 subplots (side by side)\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            \n",
    "            # Display the image\n",
    "            axes[0].imshow(image_sample, cmap='gray')\n",
    "            axes[0].set_title('Image')\n",
    "            axes[0].axis('off')  # Hide axis\n",
    "            \n",
    "            # Display the mask\n",
    "            axes[1].imshow(mask_sample, cmap='gray')\n",
    "            axes[1].set_title('Mask')\n",
    "            axes[1].axis('off')  # Hide axis\n",
    "            \n",
    "            # Display the prediction\n",
    "            axes[2].imshow(pred_sample, cmap='gray')\n",
    "            axes[2].set_title('Prediction')\n",
    "            axes[2].axis('off')  # Hide axis\n",
    "            \n",
    "            # Show the plot\n",
    "            plt.suptitle(\n",
    "                str(Datasets[i]),\n",
    "                fontsize=20,  # Font size\n",
    "                fontweight='bold',  # Font weight (e.g., 'normal', 'bold')\n",
    "                color=\"red\"  # Font color\n",
    "            )\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44996a70-32a1-4789-8d1b-dd12b0ea1cab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
