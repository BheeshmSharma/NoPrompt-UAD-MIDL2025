{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1dbb732-e4d8-4dbe-8117-86b19738564c",
   "metadata": {},
   "source": [
    "## Class for Candidate Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20e5378-c0f9-434a-9dd4-c49ceee646c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Boundary_coordinate:\n",
    "    def __init__(self, grid_size=16, image_size=256):\n",
    "        \"\"\"\n",
    "        Initialize the BoundaryMaskCreator.\n",
    "\n",
    "        Parameters:\n",
    "        - grid_size: The size of the grid for boundary point extraction\n",
    "        - image_size: The dimensions of the image (assumed square)\n",
    "        \"\"\"\n",
    "        self.grid_size = grid_size\n",
    "        self.image_size = image_size\n",
    "        self.gap = image_size // grid_size\n",
    "        self.boundary_points = self._generate_boundary_points()\n",
    "        self.boundary_points_tensor = torch.tensor(self.boundary_points, dtype=torch.float32)\n",
    "\n",
    "    def _generate_boundary_points(self):\n",
    "        \"\"\"\n",
    "         \n",
    "        \n",
    "        Returns:\n",
    "        - boundary_points: Array of boundary points\n",
    "        \"\"\"\n",
    "        boundary_points = []\n",
    "        for i in range(self.grid_size):\n",
    "            # Horizontal lines\n",
    "            y = i * self.gap\n",
    "            for x in range(0, self.image_size, self.gap):\n",
    "                boundary_points.append((x, y))\n",
    "            \n",
    "            # Vertical lines\n",
    "            x = i * self.gap\n",
    "            for y in range(0, self.image_size, self.gap):\n",
    "                boundary_points.append((x, y))\n",
    "        \n",
    "        boundary_points = np.array(boundary_points)\n",
    "        boundary_points = np.unique(boundary_points, axis=0)\n",
    "        return boundary_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ad2d4-3eea-4aa8-9a89-9832c764f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "creator = Boundary_coordinate(grid_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac9dad6-8ad1-4f40-8aa5-61bad26651a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_points = creator._generate_boundary_points()\n",
    "print(f\"boundary_points {boundary_points.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad63f7e-79c7-4141-b0b4-51d5e423591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de11634-6c5d-437d-9dd7-38daf55f2564",
   "metadata": {},
   "source": [
    "## SAM Prompt Encoder Function with Initialized Weights (Note: Pretrained weights are not used; instead, the point-based prompt embedding class functionality is utilized.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6102a-06e5-4ee8-aff5-852044a713b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import SamModel, SamConfig\n",
    "\n",
    "# Set device (use GPU if available, otherwise fallback to CPU)\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize the model from scratch with a configuration\n",
    "config = SamConfig()  # This will use the default configuration. You can modify it if needed.\n",
    "SAM = SamModel(config).to(device)\n",
    "\n",
    "# Freeze the prompt encoder's parameters\n",
    "for param in SAM.prompt_encoder.parameters():\n",
    "    param.requires_grad = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8cf0bf-a6c6-4a55-a9b7-c704f4ccb41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_coordinate = torch.tensor(boundary_points)\n",
    "print(\"point_coordinate\", point_coordinate.shape)\n",
    "\n",
    "input_boxes = torch.zeros((point_coordinate.size(0), 4), dtype=point_coordinate.dtype).to(device)\n",
    "\n",
    "# Fill the new tensor\n",
    "input_boxes[:, 0] = point_coordinate[:, 0]  # First column\n",
    "input_boxes[:, 1] = point_coordinate[:, 1]  # Second column\n",
    "input_boxes[:, 2] = point_coordinate[:, 0]  # Repeat first column\n",
    "input_boxes[:, 3] = point_coordinate[:, 1]  # Repeat second column\n",
    "\n",
    "input_boxes = input_boxes.unsqueeze(1)\n",
    "input_boxes = torch.round(input_boxes * 4)\n",
    "\n",
    "print(\"point_coordinate\", point_coordinate)\n",
    "print(\"input_boxes\", input_boxes.shape)\n",
    "print(\"input_boxes\", input_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a95491f-1543-46f2-9d6d-0fd72ef318f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_embeddings, dense_embeddings = SAM.prompt_encoder(\n",
    "    input_points = None,\n",
    "    input_boxes = input_boxes,\n",
    "    input_masks = None,\n",
    "    input_labels = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a73cd4-bcec-4fef-909d-61d0ccb9b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sparse_embeddings:::::\", sparse_embeddings.shape)\n",
    "print(\"dense_embeddings:::::\", dense_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e6243-6558-4240-ae37-f40c8ad57f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_emd = sparse_embeddings[:, 0, 0, :]\n",
    "print(\"sparse_emd:::::\", sparse_emd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6d046b-bb92-4042-a5ef-c9432804ba54",
   "metadata": {},
   "source": [
    "## Saving this location embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb995b6c-a046-4355-8c1d-77c7659273f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca86f9c-25e4-44b2-96c3-7838dfc17e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sparse_emd, './Candidate_Prompt_Embedding' + str(i) + '.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
