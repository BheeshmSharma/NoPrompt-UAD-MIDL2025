{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1dbb732-e4d8-4dbe-8117-86b19738564c",
   "metadata": {},
   "source": [
    "## Class for Candidate Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d20e5378-c0f9-434a-9dd4-c49ceee646c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Boundary_coordinate:\n",
    "    def __init__(self, grid_size=16, image_size=256):\n",
    "        \"\"\"\n",
    "        Initialize the BoundaryMaskCreator.\n",
    "\n",
    "        Parameters:\n",
    "        - grid_size: The size of the grid for boundary point extraction\n",
    "        - image_size: The dimensions of the image (assumed square)\n",
    "        \"\"\"\n",
    "        self.grid_size = grid_size\n",
    "        self.image_size = image_size\n",
    "        self.gap = image_size // grid_size\n",
    "        self.boundary_points = self._generate_boundary_points()\n",
    "        self.boundary_points_tensor = torch.tensor(self.boundary_points, dtype=torch.float32)\n",
    "\n",
    "    def _generate_boundary_points(self):\n",
    "        \"\"\"\n",
    "         \n",
    "        \n",
    "        Returns:\n",
    "        - boundary_points: Array of boundary points\n",
    "        \"\"\"\n",
    "        boundary_points = []\n",
    "        for i in range(self.grid_size):\n",
    "            # Horizontal lines\n",
    "            y = i * self.gap\n",
    "            for x in range(0, self.image_size, self.gap):\n",
    "                boundary_points.append((x, y))\n",
    "            \n",
    "            # Vertical lines\n",
    "            x = i * self.gap\n",
    "            for y in range(0, self.image_size, self.gap):\n",
    "                boundary_points.append((x, y))\n",
    "        \n",
    "        boundary_points = np.array(boundary_points)\n",
    "        boundary_points = np.unique(boundary_points, axis=0)\n",
    "        return boundary_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c4ad2d4-3eea-4aa8-9a89-9832c764f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "creator = Boundary_coordinate(grid_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac9dad6-8ad1-4f40-8aa5-61bad26651a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boundary_points (1024, 2)\n"
     ]
    }
   ],
   "source": [
    "boundary_points = creator._generate_boundary_points()\n",
    "print(f\"boundary_points {boundary_points.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dad63f7e-79c7-4141-b0b4-51d5e423591c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0],\n",
       "       [  0,   8],\n",
       "       [  0,  16],\n",
       "       ...,\n",
       "       [248, 232],\n",
       "       [248, 240],\n",
       "       [248, 248]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boundary_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56f62b0e-ab02-4e62-b290-618a68ecbe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class SimplePromptEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=384, num_pos_feats=128, input_image_size=(256, 256), num_boxes=2):\n",
    "        \"\"\"\n",
    "        A simpler version of PromptEncoder for encoding bounding box coordinates.\n",
    "\n",
    "        Arguments:\n",
    "        embed_dim -- Dimension of the embedding (e.g., 384)\n",
    "        num_pos_feats -- Number of positional features (e.g., 128)\n",
    "        input_image_size -- Size of the input image (height, width)\n",
    "        num_boxes -- Number of key points used for encoding (default: 2 for top-left and bottom-right corners)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.input_image_size = input_image_size\n",
    "        self.num_boxes = num_boxes  # Usually 2 for two corners\n",
    "\n",
    "        # Positional embedding matrix\n",
    "        self.register_buffer(\"positional_embedding\", torch.randn((2, num_pos_feats)) * embed_dim // 2)\n",
    "\n",
    "        # Learnable embeddings for each box corner\n",
    "        self.box_embeddings = nn.ModuleList([nn.Embedding(1, num_pos_feats * 2) for _ in range(num_boxes)])\n",
    "\n",
    "    def forward(self, boxes: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Embeds bounding box coordinates.\n",
    "\n",
    "        Arguments:\n",
    "        boxes -- Tensor of shape (batch_size, num_boxes, 2) containing (x, y) coordinates.\n",
    "\n",
    "        Returns:\n",
    "        Tensor of shape (batch_size, num_boxes, embed_dim).\n",
    "        \"\"\"\n",
    "        batch_size = boxes.shape[0]\n",
    "\n",
    "        # Compute positional embeddings\n",
    "        box_embedding = self.compute_positional_embedding(boxes)\n",
    "\n",
    "        # Add learnable embeddings\n",
    "        for i in range(self.num_boxes):\n",
    "            box_embedding[:, i, :] += self.box_embeddings[i].weight\n",
    "\n",
    "        return box_embedding.view(batch_size, -1, box_embedding.shape[-1])  # Reshape to match expected output\n",
    "\n",
    "    def compute_positional_embedding(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute positional embedding for input coordinates.\n",
    "    \n",
    "        Arguments:\n",
    "        coords -- Tensor of shape (batch_size, num_boxes, 2)\n",
    "    \n",
    "        Returns:\n",
    "        Tensor with encoded positional information.\n",
    "        \"\"\"\n",
    "        coords = coords.clone().to(torch.float32)  # Convert to float before division\n",
    "    \n",
    "        # Normalize coordinates to [0, 1] range\n",
    "        height, width = self.input_image_size\n",
    "        coords[:, :, 0] /= width\n",
    "        coords[:, :, 1] /= height\n",
    "    \n",
    "        # Scale to [-1, 1] range\n",
    "        coords = 2 * coords - 1\n",
    "        coords = coords @ self.positional_embedding  # Apply embedding matrix\n",
    "    \n",
    "        # Convert to sinusoidal embeddings\n",
    "        coords = 2 * np.pi * coords\n",
    "        return torch.cat([torch.sin(coords), torch.cos(coords)], dim=-1)  # Concatenate sin and cos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb1c8220-75af-4759-98c4-a6a85769f9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point_coordinate torch.Size([1024, 2])\n",
      "coords torch.Size([1024, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "point_coordinate = torch.tensor(boundary_points)\n",
    "print(\"point_coordinate\", point_coordinate.shape)\n",
    "\n",
    "input_boxes = torch.zeros((point_coordinate.size(0), 4), dtype=point_coordinate.dtype)\n",
    "\n",
    "# Fill the new tensor\n",
    "input_boxes[:, 0] = point_coordinate[:, 0]  # First column\n",
    "input_boxes[:, 1] = point_coordinate[:, 1]  # Second column\n",
    "input_boxes[:, 2] = point_coordinate[:, 0]  # Repeat first column\n",
    "input_boxes[:, 3] = point_coordinate[:, 1]  # Repeat second column\n",
    "\n",
    "coords = input_boxes.reshape(-1, 2, 2)\n",
    "\n",
    "# coords = coords.unsqueeze(1)\n",
    "print(\"coords\", coords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "020232af-0cf8-4a89-8d35-8fe5fe9bebd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse embeddings shape: torch.Size([1024, 2, 256])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the PromptEncoder model\n",
    "prompt_encoder = SimplePromptEncoder(embed_dim=256, input_image_size=(256, 256))\n",
    "\n",
    "\n",
    "# Forward pass through the model\n",
    "sparse_embeddings = prompt_encoder(boxes=coords)\n",
    "\n",
    "# Print the output shape\n",
    "print(\"Sparse embeddings shape:\", sparse_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14d21d5a-194a-4e0b-8082-1bc7d2546cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0149, -1.4758, -0.9741,  ...,  0.9460,  1.2408,  0.9130],\n",
       "         [-2.0411,  0.2696,  0.9735,  ..., -1.8496,  1.4023, -0.4489]],\n",
       "\n",
       "        [[-0.3678, -1.8584, -0.9742,  ..., -0.4367,  0.9479, -0.7941],\n",
       "         [-2.4238, -0.1131,  0.9735,  ..., -3.2323,  1.1094, -2.1560]],\n",
       "\n",
       "        [[ 0.7220, -0.7687, -0.9742,  ..., -0.7611,  0.2408, -0.0870],\n",
       "         [-1.3340,  0.9767,  0.9735,  ..., -3.5567,  0.4023, -1.4489]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.9388, -0.4758, -1.8981,  ..., -0.4367,  0.6235, -0.7941],\n",
       "         [-1.1172,  1.2696,  0.0496,  ..., -3.2323,  0.7850, -2.1560]],\n",
       "\n",
       "        [[-0.6922, -2.3997, -0.0503,  ..., -0.7611,  1.1647, -0.0870],\n",
       "         [-2.7482, -0.6543,  1.8974,  ..., -3.5567,  1.3262, -1.4489]],\n",
       "\n",
       "        [[ 0.3975, -0.7687, -1.8981,  ...,  0.8698,  1.1647,  0.6201],\n",
       "         [-1.6584,  0.9767,  0.0496,  ..., -1.9257,  1.3262, -0.7418]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5c3ef5a-388d-4bdc-8113-7c961d48370f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse_emd::::: torch.Size([1024, 256])\n"
     ]
    }
   ],
   "source": [
    "sparse_emd = sparse_embeddings[:, 0, :]\n",
    "print(\"sparse_emd:::::\", sparse_emd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6d046b-bb92-4042-a5ef-c9432804ba54",
   "metadata": {},
   "source": [
    "## Saving this location embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb995b6c-a046-4355-8c1d-77c7659273f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ca86f9c-25e4-44b2-96c3-7838dfc17e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sparse_emd, './Candidate_Prompt_Embedding' + str(i) + '.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
