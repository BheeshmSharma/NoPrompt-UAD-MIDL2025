{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1dbb732-e4d8-4dbe-8117-86b19738564c",
   "metadata": {},
   "source": [
    "## Class for Candidate Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20e5378-c0f9-434a-9dd4-c49ceee646c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Candidate_Location_Coordinate:\n",
    "    def __init__(self, grid_size=16, image_size=256):\n",
    "        \"\"\"\n",
    "        Initialize the BoundaryMaskCreator.\n",
    "\n",
    "        Parameters:\n",
    "        - grid_size: The size of the grid for boundary point extraction\n",
    "        - image_size: The dimensions of the image (assumed square)\n",
    "        \"\"\"\n",
    "        self.grid_size = grid_size\n",
    "        self.image_size = image_size\n",
    "        self.gap = image_size // grid_size\n",
    "        self.boundary_points = self._generate_candidate_location()\n",
    "        self.boundary_points_tensor = torch.tensor(self.boundary_points, dtype=torch.float32)\n",
    "\n",
    "    def _generate_candidate_location(self):\n",
    "        \"\"\"\n",
    "         \n",
    "        \n",
    "        Returns:\n",
    "        - boundary_points: Array of boundary points\n",
    "        \"\"\"\n",
    "        boundary_points = []\n",
    "        for i in range(self.grid_size):\n",
    "            # Horizontal lines\n",
    "            y = i * self.gap\n",
    "            for x in range(0, self.image_size, self.gap):\n",
    "                boundary_points.append((x, y))\n",
    "            \n",
    "            # Vertical lines\n",
    "            x = i * self.gap\n",
    "            for y in range(0, self.image_size, self.gap):\n",
    "                boundary_points.append((x, y))\n",
    "        \n",
    "        boundary_points = np.array(boundary_points)\n",
    "        boundary_points = np.unique(boundary_points, axis=0)\n",
    "        return boundary_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ad2d4-3eea-4aa8-9a89-9832c764f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "creator = Candidate_Location_Coordinate(grid_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac9dad6-8ad1-4f40-8aa5-61bad26651a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Candidate_Location = creator._generate_candidate_location()\n",
    "print(f\"Candidate_Location {Candidate_Location.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad63f7e-79c7-4141-b0b4-51d5e423591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Candidate_Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f62b0e-ab02-4e62-b290-618a68ecbe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class Candidate_location_prompt_embedding(nn.Module):\n",
    "    def __init__(self, embed_dim=384, num_pos_feats=128, input_image_size=(256, 256), num_boxes=2):\n",
    "        \"\"\"\n",
    "        A simpler version of PromptEncoder for encoding bounding box coordinates.\n",
    "\n",
    "        Arguments:\n",
    "        embed_dim -- Dimension of the embedding (e.g., 384)\n",
    "        num_pos_feats -- Number of positional features (e.g., 128)\n",
    "        input_image_size -- Size of the input image (height, width)\n",
    "        num_boxes -- Number of key points used for encoding (default: 2 for top-left and bottom-right corners)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.input_image_size = input_image_size\n",
    "        self.num_boxes = num_boxes  # Usually 2 for two corners\n",
    "\n",
    "        # Positional embedding matrix\n",
    "        self.register_buffer(\"positional_embedding\", torch.randn((2, num_pos_feats)) * embed_dim // 2)\n",
    "\n",
    "        # Learnable embeddings for each box corner\n",
    "        self.box_embeddings = nn.ModuleList([nn.Embedding(1, num_pos_feats * 2) for _ in range(num_boxes)])\n",
    "\n",
    "    def forward(self, boxes: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Embeds bounding box coordinates.\n",
    "\n",
    "        Arguments:\n",
    "        boxes -- Tensor of shape (batch_size, num_boxes, 2) containing (x, y) coordinates.\n",
    "\n",
    "        Returns:\n",
    "        Tensor of shape (batch_size, num_boxes, embed_dim).\n",
    "        \"\"\"\n",
    "        batch_size = boxes.shape[0]\n",
    "\n",
    "        # Compute positional embeddings\n",
    "        box_embedding = self.compute_positional_embedding(boxes)\n",
    "\n",
    "        # Add learnable embeddings\n",
    "        for i in range(self.num_boxes):\n",
    "            box_embedding[:, i, :] += self.box_embeddings[i].weight\n",
    "\n",
    "        return box_embedding.view(batch_size, -1, box_embedding.shape[-1])  # Reshape to match expected output\n",
    "\n",
    "    def compute_positional_embedding(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute positional embedding for input coordinates.\n",
    "    \n",
    "        Arguments:\n",
    "        coords -- Tensor of shape (batch_size, num_boxes, 2)\n",
    "    \n",
    "        Returns:\n",
    "        Tensor with encoded positional information.\n",
    "        \"\"\"\n",
    "        coords = coords.clone().to(torch.float32)  # Convert to float before division\n",
    "    \n",
    "        # Normalize coordinates to [0, 1] range\n",
    "        height, width = self.input_image_size\n",
    "        coords[:, :, 0] /= width\n",
    "        coords[:, :, 1] /= height\n",
    "    \n",
    "        # Scale to [-1, 1] range\n",
    "        coords = 2 * coords - 1\n",
    "        coords = coords @ self.positional_embedding  # Apply embedding matrix\n",
    "    \n",
    "        # Convert to sinusoidal embeddings\n",
    "        coords = 2 * np.pi * coords\n",
    "        return torch.cat([torch.sin(coords), torch.cos(coords)], dim=-1)  # Concatenate sin and cos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1c8220-75af-4759-98c4-a6a85769f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Candidate_Location = torch.tensor(Candidate_Location.clone())\n",
    "print(\"Candidate_Location\", Candidate_Location.shape)\n",
    "\n",
    "input_boxes = torch.zeros((Candidate_Location.size(0), 4), dtype=Candidate_Location.dtype)\n",
    "\n",
    "# Fill the new tensor\n",
    "input_boxes[:, 0] = Candidate_Location[:, 0]  # First column\n",
    "input_boxes[:, 1] = Candidate_Location[:, 1]  # Second column\n",
    "input_boxes[:, 2] = Candidate_Location[:, 0]  # Repeat first column\n",
    "input_boxes[:, 3] = Candidate_Location[:, 1]  # Repeat second column\n",
    "\n",
    "coords = input_boxes.reshape(-1, 2, 2)\n",
    "\n",
    "# coords = coords.unsqueeze(1)\n",
    "print(\"coords\", coords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020232af-0cf8-4a89-8d35-8fe5fe9bebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Candidate_embedding = Candidate_location_prompt_embedding(embed_dim=256, input_image_size=(256, 256))\n",
    "Candidate_embeddings = Candidate_embedding(boxes=coords)\n",
    "print(\"Candidate Location Prompt Embeddings shape:\", Candidate_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c3ef5a-388d-4bdc-8113-7c961d48370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Candidate_emb = Candidate_embeddings[:, 0, :]\n",
    "print(\"Candidate_emb:::::\", Candidate_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6d046b-bb92-4042-a5ef-c9432804ba54",
   "metadata": {},
   "source": [
    "## Saving this location embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb995b6c-a046-4355-8c1d-77c7659273f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca86f9c-25e4-44b2-96c3-7838dfc17e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Candidate_emb, './Candidate_Prompt_Embedding' + str(i) + '.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
